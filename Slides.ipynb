{
 "metadata": {
  "language": "Julia",
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Online/Active Learning + Data Streams"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Online/Active Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "![](files/images/bandits.jpg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Requirements:\n",
      "\n",
      "* Model must be able to survive cold-starts\n",
      "* Model must be ready to make up-to-date predictions after every observation\n",
      "* Model updates must be cost effective\n",
      "    * Can't reread old data\n",
      "    * Can't do too much work per observation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Data Streams"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Many data sets are now too large to fit in RAM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "But most software assumes data can be processed in batch-mode"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "How do we process data that can't fit in memory?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "* Use algorithms that require a small memory footprint\n",
      "* Stream data one observation at a time"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Easily calculated basic statistics using O(1) memory:\n",
      "\n",
      "* Mean\n",
      "* Variance\n",
      "* Skewness\n",
      "* Kurtosis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Much harder, but approximations possible, without O(n) memory:\n",
      "\n",
      "* Median\n",
      "* Unique item count\n",
      "* Other problems with probabilistic data structure solutions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "But what about more complex models?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "We fit them using Stochastic Gradient Descent (SGD)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's build up to the full SGD idea by thinking about means"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Tracking Means"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$$\n",
      "m = \\frac{1}{n} \\sum_{i = 1}^{n} x_i\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function mean(x::Vector)\n",
      "    m = 0.0\n",
      "    n = length(x)\n",
      "    for i in 1:n\n",
      "        m += x[i]\n",
      "    end\n",
      "    return m / n\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "# methods for generic function mean\n",
        "mean(x::Array{T,1}) at In[1]:2"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean([1, 2, 3, 4])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "2.5"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$$\n",
      "m_n = \\frac{n - 1}{n} m_{n - 1} + \\frac{1}{n} x_n\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function mean(x::Vector)\n",
      "    m = 0.0\n",
      "    n = 0\n",
      "    for i in 1:length(x)\n",
      "        n += 1\n",
      "        m = ((n - 1) / n) * m + (1 / n) * x[i]\n",
      "    end\n",
      "    return m\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "# methods for generic function mean\n",
        "mean(x::Array{T,1}) at In[3]:2"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean([1, 2, 3, 4])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "2.5"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$$\n",
      "m_n = (1 - \\alpha_n) m_{n - 1} + \\alpha_n x_n\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function mean(x::Vector)\n",
      "    m = 0.0\n",
      "    n = 0\n",
      "    for i in 1:length(x)\n",
      "        n += 1\n",
      "        \u03b1 = 1 / n\n",
      "        m = (1 - \u03b1) * m + \u03b1 * x[i]\n",
      "    end\n",
      "    return m\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "# methods for generic function mean\n",
        "mean(x::Array{T,1}) at In[5]:2"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean([1, 2, 3, 4])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "2.5"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$$\n",
      "m_n = (1 - \\alpha) m_{n - 1} + \\alpha x_n\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function mean(x::Vector)\n",
      "    m = 0.0\n",
      "    \u03b1 = 0.2\n",
      "    for i in 1:length(x)\n",
      "        m = (1 - \u03b1) * m + \u03b1 * x[i]\n",
      "    end\n",
      "    return m\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "# methods for generic function mean\n",
        "mean(x::Array{T,1}) at In[7]:2"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean([1, 2, 3, 4])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "1.6384"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Equivalent to exponentially weighted moving average"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "![](files/images/means.jpg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's rewrite that last equation:\n",
      "\n",
      "$$\n",
      "m_n = (1 - \\alpha) m_{n - 1} + \\alpha x_n \\Rightarrow m_n = m_{n - 1} + \\alpha (x_n - m_{n - 1})\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "This is actually a kind of gradient descent step:\n",
      "\n",
      "$$\n",
      "L(m) = \\frac{1}{2} \\sum_{i = 1}^{n} (x_i - m)^2 \\Rightarrow \\nabla L(m) = \\sum_{i = 1}^{n} (x_i - m) (-1)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$$\n",
      "m_n = m_{n - 1} + \\alpha (x_n - m_{n - 1}) \\approx m_{n - 1} - \\alpha \\nabla L(m_{n - 1})\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "We only look at the gradient contribution from $x_i$ alone"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "This is SGD for tracking means"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "It is also exactly equivalent to gradient descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Gradient over all observations is sum of per-observation gradients"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Full SGD algorithm introduces random selection of observations"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "SGD"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's build up to the SGD for linear regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$$\n",
      "L(\\beta) = \\frac{1}{2} ||y - X^T \\beta||_2\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$$\n",
      "L(\\beta) = \\frac{1}{2} \\sum_{i = 1}^{n} (y_i - X_i^T \\beta)^2\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$$\n",
      "\\nabla L(\\beta) = \\sum_{i = 1}^{n} (y_i - X_i^T \\beta) X_i\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Gradient Descent:\n",
      "\n",
      "$$\n",
      "\\hat{\\beta}_t = \\hat{\\beta}_{t - 1} - \\eta \\nabla L(\\beta; X, y)\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function gd_pass!(X::Matrix,\n",
      "\t              y::Vector,\n",
      "\t              b::Vector,\n",
      "\t              \u03b7::Real,\n",
      "\t              gr::Vector)\n",
      "\tp, n = size(X)\n",
      "\tse = 0.0\n",
      "\tfill!(gr, 0.0)\n",
      "\tfor j in 1:n\n",
      "\t\th = dot(X[:, j], b)\n",
      "\t\tr = (y[j] - h)\n",
      "\t\tse += r^2\n",
      "\t\tgr[:] += r * X[:, j]\n",
      "\tend\n",
      "\tb[:] += \u03b7 * gr\n",
      "\treturn se\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "# methods for generic function gd_pass!\n",
        "gd_pass!(X::Array{T,2},y::Array{T,1},b::Array{T,1},\u03b7::Real,gr::Array{T,1}) at In[9]:6"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function gd(X::Matrix,\n",
      "\t        y::Vector,\n",
      "\t        \u03b7::Real = 0.001,\n",
      "\t        max_itr::Integer = 1_000)\n",
      "\tp, n = size(X)\n",
      "\tb = zeros(p)\n",
      "\tgr = zeros(p)\n",
      "\tse, se_old = Inf, Inf\n",
      "\titr = 0\n",
      "\twhile !(abs(se - se_old) < 10e-6) && itr < max_itr\n",
      "\t\titr += 1\n",
      "\t\tse_old = se\n",
      "\t\tse = gd_pass!(X, y, b, \u03b7, gr)\n",
      "\tend\n",
      "\treturn b\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "# methods for generic function gd\n",
        "gd(X::Array{T,2},y::Array{T,1}) at In[10]:5\n",
        "gd(X::Array{T,2},y::Array{T,1},\u03b7::Real) at In[10]:5\n",
        "gd(X::Array{T,2},y::Array{T,1},\u03b7::Real,max_itr::Integer) at In[10]:5"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "SGD Trick: Use a single, randomly drawn observation to approximate gradient"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "\n",
      "$$\n",
      "\\hat{\\beta}_t = \\hat{\\beta}_{t - 1} + \\eta (y_i - X_i^T \\beta) X_i\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function sgd_pass!(X::Matrix,\n",
      "\t               y::Vector,\n",
      "\t               b::Vector,\n",
      "\t               \u03b7::Real,\n",
      "\t               gr::Vector)\n",
      "\tp, n = size(X)\n",
      "\tj = rand(1:n)\n",
      "\th = dot(X[:, j], b)\n",
      "\tr = (y[j] - h)\n",
      "\tgr[:] = r * X[:, j]\n",
      "\tb[:] += \u03b7 * gr\n",
      "\treturn\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "# methods for generic function sgd_pass!\n",
        "sgd_pass!(X::Array{T,2},y::Array{T,1},b::Array{T,1},\u03b7::Real,gr::Array{T,1}) at In[11]:6"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function sgd(X::Matrix,\n",
      "\t         y::Vector,\n",
      "\t         \u03b7::Real = 0.001,\n",
      "\t         max_itr::Integer = 1_000)\n",
      "\tp, n = size(X)\n",
      "\tb = zeros(p)\n",
      "\tgr = zeros(p)\n",
      "\titr = 0\n",
      "\twhile itr < max_itr\n",
      "\t\titr += 1\n",
      "\t\tsgd_pass!(X, y, b, \u03b7, gr)\n",
      "\tend\n",
      "\treturn b\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "# methods for generic function sgd\n",
        "sgd(X::Array{T,2},y::Array{T,1}) at In[12]:5\n",
        "sgd(X::Array{T,2},y::Array{T,1},\u03b7::Real) at In[12]:5\n",
        "sgd(X::Array{T,2},y::Array{T,1},\u03b7::Real,max_itr::Integer) at In[12]:5"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Other easy examples:\n",
      "\n",
      "* Logistic regression\n",
      "* SVD-like matrix factorization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "invlogit(z) = 1 / (1 + exp(-z))\n",
      "\n",
      "function logit_sgd_pass!(X::Matrix,\n",
      "\t               y::Vector,\n",
      "\t               b::Vector,\n",
      "\t               \u03b7::Real,\n",
      "\t               gr::Vector)\n",
      "\tp, n = size(X)\n",
      "\tj = rand(1:n)\n",
      "\th = dot(X[:, j], b)\n",
      "\tr = (y[j] - invlogit(h))\n",
      "\tgr[:] = r * X[:, j]\n",
      "\tb[:] += \u03b7 * gr\n",
      "\treturn\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "# methods for generic function logit_sgd_pass!\n",
        "logit_sgd_pass!(X::Array{T,2},y::Array{T,1},b::Array{T,1},\u03b7::Real,gr::Array{T,1}) at In[13]:8"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function factorize(ratings::Matrix{Int64},\n",
      "\t\t\t\t   f::Integer,\n",
      "\t\t\t\t   gamma::Real,\n",
      "\t\t\t\t   lambda::Real,\n",
      "\t\t\t\t   passes::Integer)\n",
      "\t# Determine the input size\n",
      "\tn_users = max(ratings[:, 1])\n",
      "\tn_items = max(ratings[:, 2])\n",
      "\tn = size(train_ratings, 1)\n",
      "\n",
      "\t# Initialize random vectors\n",
      "\tp = randn(f, n_users)\n",
      "\tq = randn(f, n_items)\n",
      "\n",
      "\t# Make several SGD passes through the data\n",
      "\tfor pass in 1:passes\n",
      "\t\t# Iterate over all entries\n",
      "\t\tfor row in 1:n_train\n",
      "\t\t\t# Determine values\n",
      "\t\t\tuser, item, rating = train_ratings[row, 1:3]\n",
      "\n",
      "\t\t\t# Calculate residual\n",
      "\t\t\te_ui = rating - dot(p[:, user], q[:, item])\n",
      "\n",
      "\t\t\t# Alternate SGD steps for q and p\n",
      "\t\t\tfor dim in 1:f\n",
      "\t\t\t\tq[dim, item] = q[dim, item] + gamma * (e_ui * p[dim, user] - lambda * q[dim, item])\n",
      "\t\t\tend\n",
      "\t\t\tfor dim in 1:f\n",
      "\t\t\t\tp[dim, user] = p[dim, user] + gamma * (e_ui * q[dim, item] - lambda * p[dim, user])\n",
      "\t\t\tend\n",
      "\t\tend\n",
      "\tend\n",
      "\n",
      "\t# Return factorization\n",
      "\treturn p, q\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "# methods for generic function factorize\n",
        "factorize(ratings::Array{Int64,2},f::Integer,gamma::Real,lambda::Real,passes::Integer) at In[14]:7"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Problems with SGD"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "* Constant step-size $\\eta$ must be tuned\n",
      "    * Good batch algorithms use line search algorithms\n",
      "* High-precision convergence very slow compared with quasi-Newton methods\n",
      "    * Problems with complex Hessians are hard to solve\n",
      "* Need to be very careful to ensure stable solutions\n",
      "    * Big steps at the start will spiral out of control"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "But sometimes low-precision convergence is a virtue"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bad optimization algorithms can make good learning algorithms"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Improving SGD"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "* Decreasing step-sizes over time\n",
      "* Smooth estimates by averaging\n",
      "* Set learning rates per parameter"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Decreasing step-sizes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Consider the \"optimal\" mean inference rule"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$$\n",
      "\\alpha_n = \\frac{1}{n}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "In general, convergence requires that $\\alpha$ decrease like $\\frac{1}{n}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "One popular rule:\n",
      "\n",
      "$$\n",
      "\\alpha_n = \\frac{\\eta}{\\tau_0 + n}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Averaging estimates"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function polyak_pass!(X::Matrix,\n",
      "\t                  y::Vector,\n",
      "\t                  b::Vector,\n",
      "\t                  \u03b7::Real,\n",
      "\t                  gr::Vector)\n",
      "\tp, n = size(X)\n",
      "\tj = rand(1:n)\n",
      "\th = dot(X[:, j], b)\n",
      "\tr = (y[j] - h)\n",
      "\tfor i in 1:p\n",
      "\t\tgr[i] = r * X[i, j]\n",
      "\t\tb[i] += \u03b7 * gr[i]\n",
      "\tend\n",
      "\treturn\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "# methods for generic function polyak_pass!\n",
        "polyak_pass!(X::Array{T,2},y::Array{T,1},b::Array{T,1},\u03b7::Real,gr::Array{T,1}) at In[15]:6"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fit Polyak-Ruppert averaging\n",
      "function polyak(X::Matrix,\n",
      "\t            y::Vector,\n",
      "\t            \u03b7::Real = 0.001,\n",
      "\t            max_itr::Integer = 1_000)\n",
      "\tp, n = size(X)\n",
      "\tb = zeros(p)\n",
      "\tc = zeros(p)\n",
      "\tgr = zeros(p)\n",
      "\titr = 0\n",
      "\twhile itr < max_itr\n",
      "\t\titr += 1\n",
      "\t\tpolyak_pass!(X, y, b, \u03b7, gr)\n",
      "\t\tc[:] = ((itr - 1) / itr) * c + (1 / itr) * b\n",
      "\tend\n",
      "\treturn c\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "# methods for generic function polyak\n",
        "polyak(X::Array{T,2},y::Array{T,1}) at In[16]:6\n",
        "polyak(X::Array{T,2},y::Array{T,1},\u03b7::Real) at In[16]:6\n",
        "polyak(X::Array{T,2},y::Array{T,1},\u03b7::Real,max_itr::Integer) at In[16]:6"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Per-parameter step-sizes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function adagrad_pass!(X::Matrix,\n",
      "\t                   y::Vector,\n",
      "\t                   b::Vector,\n",
      "\t                   \u03b7::Real,\n",
      "\t                   gr::Vector,\n",
      "\t                   s::Vector,\n",
      "\t                   \u03c40::Integer)\n",
      "\tp, n = size(X)\n",
      "\tj = rand(1:n)\n",
      "\th = dot(X[:, j], b)\n",
      "\tr = (y[j] - h)\n",
      "\tfor i in 1:p\n",
      "\t\tgr[i] = r * X[i, j]\n",
      "\t\ts[i] += gr[i]^2\n",
      "\t\tb[i] += \u03b7 * gr[i] / (\u03c40 + sqrt(s[i]))\n",
      "\tend\n",
      "\treturn\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "# methods for generic function adagrad_pass!\n",
        "adagrad_pass!(X::Array{T,2},y::Array{T,1},b::Array{T,1},\u03b7::Real,gr::Array{T,1},s::Array{T,1},\u03c40::Integer) at In[17]:8"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function adagrad(X::Matrix,\n",
      "\t             y::Vector,\n",
      "\t             \u03b7::Real = 0.001,\n",
      "\t             max_itr::Integer = 1_000,\n",
      "\t             \u03c40::Integer = 0)\n",
      "\tp, n = size(X)\n",
      "\tb = zeros(p)\n",
      "\tgr = zeros(p)\n",
      "\ts = zeros(p)\n",
      "\titr = 0\n",
      "\twhile itr < max_itr\n",
      "\t\titr += 1\n",
      "\t\tadagrad_pass!(X, y, b, \u03b7, gr, s, \u03c40)\n",
      "\tend\n",
      "\treturn b\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "# methods for generic function adagrad\n",
        "adagrad(X::Array{T,2},y::Array{T,1}) at In[18]:6\n",
        "adagrad(X::Array{T,2},y::Array{T,1},\u03b7::Real) at In[18]:6\n",
        "adagrad(X::Array{T,2},y::Array{T,1},\u03b7::Real,max_itr::Integer) at In[18]:6\n",
        "adagrad(X::Array{T,2},y::Array{T,1},\u03b7::Real,max_itr::Integer,\u03c40::Integer) at In[18]:6"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Other topics:\n",
      "    \n",
      "* Variants of quasi-Newton methods\n",
      "* Automatic learning rate tuning\n",
      "* Selecting number of passes (epochs) through data"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}